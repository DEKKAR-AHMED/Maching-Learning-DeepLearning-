ylab('Salary')
# RAndom Forest
dataset = read.csv("/home/ahmed/Desktop/machine-learning-A-Z/Regression Probleme/RandomForest/Position_Salaries.csv")
dataset
dataset = dataset[2:3]
# fittin the random forest model
library(randomForest)
set.seed(1234)
randomfor = randomForest(x = dataset[1] # give a dataframe
,y = dataset$Salary, # give me an array
ntree = 100)
y_pred = predict(randomfor, data.frame(Level = 6.5))
y_pred
# Visualising the Regression Model results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.001)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(randomfor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Regression Model)') +
xlab('Level') +
ylab('Salary')
# fittin the random forest model
library(randomForest)
set.seed(1234)
randomfor = randomForest(x = dataset[1] # give a dataframe
,y = dataset$Salary, # give me an array
ntree = 100)
y_pred = predict(randomfor, data.frame(Level = 6.5))
y_pred
# RAndom Forest
dataset = read.csv("/home/ahmed/Desktop/machine-learning-A-Z/Regression Probleme/RandomForest/Position_Salaries.csv")
dataset
dataset = dataset[2:3]
# fittin the random forest model
library(randomForest)
set.seed(1234)
randomfor = randomForest(x = dataset[1] # give a dataframe
,y = dataset$Salary, # give me an array
ntree = 600)
y_pred = predict(randomfor, data.frame(Level = 6.5))
y_pred
# Visualising the Regression Model results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.001)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(randomfor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Regression Model)') +
xlab('Level') +
ylab('Salary')
install.packages("FactoMineR")
install.packages("FactoMineR")
install.packages("~/Downloads/FactoMineR_2.3.tgz", repos = NULL, type = .Platform$pkgType)
install.packages(c("cli", "ggplot2", "gplots", "lava", "plyr", "rlang", "xlsx"))
install.packages("FactoMineR")
install.packages("chron")
FactoMineR
install.packages("FactoMineR")
install.packages("FactoMinerR")
install.packages("FactoMineR")
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
# the number times  of ad i is selected in round n
n_selected_ads = integer(d)
# the number  of reward of ad i in the round n
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_0[i]+1,shape2 = sum_of_rewards_1[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
# the number times  of ad i is selected in round n
n_selected_ads = integer(d)
# the number  of reward of ad i in the round n
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_0[i]+1,shape2 = sum_of_rewards_1[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# Thompson sampling
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Reinforcement Learning/Tompson Sampling/Ads_CTR_Optimisation.csv")
# implimenting Thonmpson sampling algorithme
d = 10
N= 10000
total_rewards= 0
sum_of_rewards_0= integer(d)
sum_of_rewards_1= integer(d)
ads_selected= integer()
for(n in 1:N){
max_random = 0
ad = 0
for (i in 1:d){
randomBeta = rbeta(n = 1,shape1 = sum_of_rewards_1[i]+1,shape2 = sum_of_rewards_0[i]+1)
if (randomBeta > max_random){
max_random = randomBeta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if ( reward == 0){
sum_of_rewards_0[ad] =  sum_of_rewards_0[ad] + 1;
}else{
sum_of_rewards_1[ad] = sum_of_rewards_1[ad]+ 1 ;
}
total_rewards = total_rewards + reward
}
print(total_rewards)
# visualisation
hist(ads_selected,col = "blue",)
# k means
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv
")
# k means
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
View(x)
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
sum(kmeans(x,i)$withiness)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withiness)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans(x,5,iter.max = 300,nstart = 10)
library(cluster, lib.loc = "/usr/lib/R/library")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans(x,5,iter.max = 300,nstart = 10)
# visualisation cluset
#library(cluster)
clusplot(x,kmeans$cluster,lines=0,shade = TRUE,labels = 2,plotchar = FALSE,span = TRUE,main = paste("cluster of client"),xlab = "annual income",ylab = "spending score")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans(x,5,iter.max = 300,nstart = 10)
# visualisation cluset
#library(cluster)
clusplot(x,kmeans$cluster,lines=0,shade = TRUE,labels = 2,plotchar = FALSE,span = TRUE,main = paste("cluster of client"),xlab = "annual income",ylab = "spending score")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans = kmeans(x,5,iter.max = 300,nstart = 10)
# visualisation cluset
#library(cluster)
clusplot(x,kmeans$cluster,lines=0,shade = TRUE,labels = 2,plotchar = FALSE,span = TRUE,main = paste("cluster of client"),xlab = "annual income",ylab = "spending score")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans = kmeans(x,5,iter.max = 300,nstart = 10)
# visualisation cluset
#library(cluster)
clusplot(x,kmeans$cluster,lines=0,shade = TRUE,labels = 2,plotchar = FALSE,color = TRUE ,span = TRUE,main = paste("cluster of client"),xlab = "annual income",ylab = "spending score")
# k means
# import the dataset
dataset = read.csv("/home/ahmed/Desktop/MachinLearning-DeepLearning/Clustering/K-Means Clustering/Mall_Customers.csv")
x = dataset[4:5]
# identify the number of cluster the elbow model
set.seed(6) # to fix the number of random for all user
wcss = vector()
for(i in 1:10){
wcss[i]= sum(kmeans(x,i)$withinss)
}
plot(1:10,wcss,type="b",main=paste("cluster of Client"),xlab = "number of cluster",ylab = "WCSS")
# the optimal cluster is 5
# applaying kmeans
set.seed(29)
kmeans = kmeans(x,5,iter.max = 300,nstart = 10)
# visualisation cluset
#library(cluster)
clusplot(x,kmeans$cluster,lines=0,shade = TRUE,labels = 2,plotchar = FALSE,color = TRUE ,span = TRUE,main = paste("cluster of client"),xlab = "annual income",ylab = "spending score")
setwd("/home/ahmed/Desktop/MachinLearning-DeepLearning/Association Rule Learning/Eclat/")
setwd("/home/ahmed/Desktop/MachinLearning-DeepLearning/Association Rule Learning/Eclat/")
# import data
data_set = read.csv("Market_Basket_Optimisation.csv")
setwd("/home/ahmed/Desktop/MachinLearning-DeepLearning/Association Rule Learning/Eclat/")
# import data
data_set = read.csv("Market_Basket_Optimisation.csv")
View(data_set)
setwd("/home/ahmed/Desktop/MachinLearning-DeepLearning/Association Rule Learning/Eclat/")
# import data
data_set = read.csv("Market_Basket_Optimisation.csv")
# Eclat
# in the priori algorithme will not use csv data but we will use sparse matrxi that contain a lot of zero
# binary metrix
# to create the spase matrix  to do that we have to install the arules library
library(arules)
# as we will no t use our csv so wi will delete the data set
dataset = read.transactions("/home/ahmed/Desktop/MachinLearning-DeepLearning/Association Rule Learning/Apriori/Market_Basket_Optimisation.csv",sep = ",",rm.duplicates = TRUE)
dataset
summary(dataset)
itemFrequencyPlot(dataset,topN = 10)
#first step choose a minimum for the support it depends on the busnis problem
# we considere that the product bought 3 time per day or 21 time in a week
sup = 28/7500
# confidence = 0.8 default sup we have done the computation
rules= eclat(data = dataset,parameter = list(support = 0.004 , minlen = 2))
rules= eclat(data = dataset,parameter = list(support = 0.004 , minlen = 2))
# lets view the details and conclude
# th e most important information we will must conclude is the numbe
# of sets
# the step2 and three are being executed by the apriori function
# the step for sort the rules bu the decreasing lift
inspect(sort(rules,by= "support")[1:10])
# visualisation the result
